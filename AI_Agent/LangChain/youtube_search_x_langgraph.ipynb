{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vi1FHt-dxzDb"
      },
      "outputs": [],
      "source": [
        "#@title Setup Environment { display-mode: \"form\" }\n",
        "\n",
        "!pip install langgraph\n",
        "!pip install -U langchain langchain_openai langchainhub tavily-python\n",
        "!pip3 install google-api-python-client\n",
        "# Youtube API Client\n",
        "!pip3 install youtube_transcript_api"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define the YoutubeSearchTool { display-mode: \"form\" }\n",
        "\n",
        "from langchain.tools.base import BaseTool\n",
        "from googleapiclient.discovery import build\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "import json\n",
        "from pydantic import Field\n",
        "\n",
        "\n",
        "class YoutubeSearchTool(BaseTool):\n",
        "    \"\"\"Tool that fetches search results from YouTube.\"\"\"\n",
        "\n",
        "    name: str = \"YoutubeSearchTool\"\n",
        "    youtube_api_key: str = Field(...,\n",
        "                                 description=\"API key for accessing Youtube data.\")\n",
        "    description: str = (\n",
        "        \"A tool that fetches search results from YouTube based on a query.\\n\"\n",
        "        \"Arguments:\\n\"\n",
        "        \"- query: The search term to look for on YouTube.\\n\"\n",
        "        \"- youtube_api_key: The API key to access YouTube data.\\n\\n\"\n",
        "        \"Output Format:\\n\"\n",
        "        \"- Title: Displayed after translation to Japanese.\\n\"\n",
        "        \"- first_280_chars_of_transcript:This field contains the first 280 characters of the video's transcript.\\n\"\n",
        "        \"- viewCount: Number of times the video has been viewed.\\n\"\n",
        "        \"- likeCount: Number of likes the video has received.\\n\"\n",
        "        \"- Description: Displayed after translation to Japanese.\\n\"\n",
        "        \"- Published Date: Displayed as 'publishedAt'.\\n\"\n",
        "        \"- Video Link: Formatted as https://www.youtube.com/watch?v={video_id}.\"\n",
        "    )\n",
        "\n",
        "    def __init__(self, youtube_api_key: str, *args, **kwargs):\n",
        "        if not youtube_api_key:\n",
        "            raise ValueError(\"A valid Youtube developer key must be provided.\")\n",
        "        kwargs[\"youtube_api_key\"] = youtube_api_key\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def _run(self, q: str, max_results: int = 100) -> str:\n",
        "        YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
        "        YOUTUBE_API_VERSION = \"v3\"\n",
        "        youtube = build(YOUTUBE_API_SERVICE_NAME,\n",
        "                        YOUTUBE_API_VERSION, developerKey=self.youtube_api_key)\n",
        "\n",
        "        search_response = youtube.search().list(\n",
        "            q=q,\n",
        "            part=\"id,snippet\",\n",
        "            order='date',  # Sort by published date\n",
        "            type='video',\n",
        "            maxResults=max_results\n",
        "        ).execute()\n",
        "\n",
        "        videos = search_response['items']\n",
        "        video_list = []\n",
        "\n",
        "        for video in videos:\n",
        "            video_data = {}\n",
        "            video_id = video['id']['videoId']\n",
        "            video_data['video_id'] = video_id\n",
        "            video_data['title'] = video['snippet']['title']\n",
        "            video_data['publishedAt'] = video['snippet']['publishedAt']\n",
        "            video_data['description'] = video['snippet']['description']\n",
        "\n",
        "            # Fetch viewCount and likeCount for each video\n",
        "            video_response = youtube.videos().list(\n",
        "                part=\"statistics\",\n",
        "                id=video_id\n",
        "            ).execute()\n",
        "            statistics = video_response[\"items\"][0][\"statistics\"]\n",
        "            video_data['viewCount'] = statistics.get(\"viewCount\", \"0\")\n",
        "            video_data['likeCount'] = statistics.get(\"likeCount\", \"0\")\n",
        "\n",
        "            # Only add videos with more than 1000 views to the list\n",
        "            if int(video_data['viewCount']) >= 1000:\n",
        "                video_list.append(video_data)\n",
        "\n",
        "        # Sort the video list by 'publishedAt' in descending order and take the first 5\n",
        "        latest_5_videos = sorted(\n",
        "            video_list, key=lambda x: x['publishedAt'], reverse=True)[:5]\n",
        "\n",
        "        # Get first 280 characters of transcript for each video\n",
        "        for video in latest_5_videos:\n",
        "            video_id = video['video_id']\n",
        "            try:\n",
        "                transcript = YouTubeTranscriptApi.get_transcript(\n",
        "                    video_id, languages=['en', 'ja'])\n",
        "                transcript_text = [entry['text'] for entry in transcript]\n",
        "                transcript_string = ' '.join(transcript_text)\n",
        "                first_280_chars = transcript_string[:280]\n",
        "                video['first_280_chars_of_transcript'] = first_280_chars\n",
        "            except:\n",
        "                video['first_280_chars_of_transcript'] = \"Transcript not available\"\n",
        "\n",
        "        # Convert to JSON format\n",
        "        items_json = json.dumps(latest_5_videos)\n",
        "        return items_json\n",
        "\n",
        "    async def _arun(self, q: str, max_results: int = 100) -> str:\n",
        "        \"\"\"Use the YoutubeSearchTool asynchronously.\"\"\"\n",
        "        return self._run(q, max_results)"
      ],
      "metadata": {
        "id": "cYKcDoLnxz3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set API keys as Environment Variables\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# OpenAI and Tavily API\n",
        "openai_api_key = 'OPENAI_API_KEY'  # @param {type: \"string\"}\n",
        "tavily_api_key = 'TAVILY_API_KEY'  # @param {type: \"string\"}\n",
        "# LangSmith\n",
        "langchain_tracing_v2 = 'true'  # @param {type: \"string\"}\n",
        "langchain_api_key = 'LANGCHAIN_API_KEY'  # @param {type: \"string\"}\n",
        "youtube_api_key= 'YOUTUBE_API_KEY'   # @param {type: \"string\"}\n",
        "\n",
        "# Check if environment variables are set and display error messages\n",
        "if not openai_api_key:\n",
        "    print(\"OPENAI_API_KEY is not set.\")\n",
        "if not tavily_api_key:\n",
        "    print(\"TAVILY_API_KEY is not set.\")\n",
        "if not langchain_tracing_v2:\n",
        "    print(\"LANGCHAIN_TRACING_V2 is not set.\")\n",
        "if not langchain_api_key:\n",
        "    print(\"LANGCHAIN_API_KEY is not set.\")\n",
        "if not youtube_api_key:\n",
        "    print(\"YOUTUBE_API_KEY is not set.\")\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "os.environ['TAVILY_API_KEY'] = tavily_api_key\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = langchain_tracing_v2\n",
        "os.environ['LANGCHAIN_API_KEY'] = langchain_api_key\n",
        "os.environ['YOUTUBE_API_KEY'] = youtube_api_key\n",
        "\n"
      ],
      "metadata": {
        "id": "jUqCIJOgx0Br",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Integrating AI Tools into a Unified Workflow with LangChain\n",
        "\n",
        "# Import necessary libraries and\n",
        "from langchain import hub\n",
        "from langchain.agents import create_openai_functions_agent\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.agents import AgentFinish\n",
        "from langgraph.graph import END, Graph\n",
        "import os\n",
        "\n",
        "# Define the tools to be used\n",
        "tools = [\n",
        "    TavilySearchResults(max_results=1),\n",
        "    YoutubeSearchTool(name=\"YoutubeSearch\", youtube_api_key=os.environ.get('YOUTUBE_API_KEY'))\n",
        "]\n",
        "\n",
        "# Retrieve the prompt to be used\n",
        "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
        "\n",
        "# Define the model to be used\n",
        "model = 'gpt-3.5-turbo-1106'  # @param {type: \"string\"}\n",
        "\n",
        "# Choose the Large Language Model (LLM)\n",
        "llm = ChatOpenAI(model=model)\n",
        "\n",
        "# Construct the OpenAI Functions agent\n",
        "agent_runnable = create_openai_functions_agent(llm, tools, prompt)\n",
        "\n",
        "# Define the agent\n",
        "agent = RunnablePassthrough.assign(agent_outcome=agent_runnable)\n",
        "\n",
        "# Define the function to execute tools\n",
        "def execute_tools(data):\n",
        "    agent_action = data.pop('agent_outcome')\n",
        "    tool_to_use = {t.name: t for t in tools}[agent_action.tool]\n",
        "    observation = tool_to_use.invoke(agent_action.tool_input)\n",
        "    data['intermediate_steps'].append((agent_action, observation))\n",
        "    return data\n",
        "\n",
        "# Define the logic to decide whether to continue\n",
        "def should_continue(data):\n",
        "    if isinstance(data['agent_outcome'], AgentFinish):\n",
        "        return \"exit\"\n",
        "    else:\n",
        "        return \"continue\"\n",
        "\n",
        "# Build the workflow\n",
        "workflow = Graph()\n",
        "workflow.add_node(\"agent\", agent)\n",
        "workflow.add_node(\"tools\", execute_tools)\n",
        "workflow.set_entry_point(\"agent\")\n",
        "workflow.add_conditional_edges(\"agent\", should_continue, {\"continue\": \"tools\", \"exit\": END})\n",
        "workflow.add_edge('tools', 'agent')\n",
        "\n",
        "# Compile into a LangChain Runnable\n",
        "chain = workflow.compile()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NrT1TBu_x0FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Workflow With Text\n",
        "text = \"Search and collect articles and  videos about LangChain\"  # @param {type: \"string\"}\n",
        "result = chain.invoke({\"input\": text, \"intermediate_steps\": []})\n",
        "result['agent_outcome'].return_values['output']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "cellView": "form",
        "id": "OY7dKccPx0MZ",
        "outputId": "681a60ec-c155-4e5a-d1a7-df2eda406839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Here are some articles and videos about LangChain:\\n\\n### Articles\\n1. [LangChain - Chains](https://blog.paperspace.com/langchain/)\\n   - LangChain is a versatile interface for various Language Model Models (LLMs), offering a centralized development. The article discusses the possibilities of chatGPT and other LLMs in our lives and how LangChain can be used to build powerful AI models.\\n\\n### Videos\\n1. [What Is Retrieval Augmented Generation (RAG)](https://www.youtube.com/watch?v=yj4-3SALiIk)\\n   - Published Date: February 9, 2024\\n   - Description: Explains Retrieval Augmented Generation and how it can be used to build powerful AI apps with large language models like chatGPT.\\n   - Views: 1362, Likes: 117\\n\\n2. [Gemini + Google Retrieval Agent from a LangChain Template](https://www.youtube.com/watch?v=rZus0JtRqXE)\\n   - Published Date: February 9, 2024\\n   - Description: Demonstrates the use of langchain-cli to quickly bootstrap a LangChain agent using Google's `gemini-pro` model.\\n   - Views: 2180, Likes: 65\\n\\n3. [Building a Real-Time Voice Bot with Deepgram, Langchain, and ChatGPT](https://www.youtube.com/watch?v=EgNerWaeZz0)\\n   - Published Date: February 8, 2024\\n   - Description: Shows an experiment example of a real-time voice bot using Websockets, Deepgram for speech recognition, and LangChain.\\n   - Views: 1148, Likes: 2\\n\\n4. [What Is LangChain and When to Use It](https://www.youtube.com/watch?v=PG3FQQVG06k)\\n   - Published Date: February 8, 2024\\n   - Description: Discusses LangChain as a framework to build powerful AI apps using LLMs like ChatGPT and when it should be used.\\n   - Views: 2076, Likes: 167\\n\\n5. [RAG Evaluation with LangChain v0.1.0 and RAGAS (RAG ASessment)](https://www.youtube.com/watch?v=Anr1br0lLz8)\\n   - Published Date: February 7, 2024\\n   - Description: Explores the critical art of evaluating and improving production with LangChain.\\n   - Views: 1395, Likes: 97\\n\\nFeel free to explore these resources to learn more about LangChain!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}